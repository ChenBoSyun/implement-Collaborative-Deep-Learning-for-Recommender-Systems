{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "#init random seed\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find vocabulary_size = 8000\n",
    "with open(r\"ctrsr_datasets/citeulike-a/vocabulary.dat\") as vocabulary_file:\n",
    "    vocabulary_size = len(vocabulary_file.readlines())\n",
    "    \n",
    "#find item_size = 16980\n",
    "with open(r\"ctrsr_datasets/citeulike-a/mult.dat\") as item_info_file:\n",
    "    item_size = len(item_info_file.readlines())\n",
    "\n",
    "#initialize item_infomation_matrix (16980 , 8000)\n",
    "item_infomation_matrix = np.zeros((item_size , vocabulary_size))\n",
    "\n",
    "#build item_infomation_matrix\n",
    "with open(r\"ctrsr_datasets/citeulike-a/mult.dat\") as item_info_file:\n",
    "    sentences = item_info_file.readlines()\n",
    "    \n",
    "    for index,sentence in enumerate(sentences):\n",
    "        words = sentence.strip().split(\" \")[1:]\n",
    "        for word in words:\n",
    "            vocabulary_index , number = word.split(\":\")\n",
    "            item_infomation_matrix[index][int(vocabulary_index)] =number\n",
    "\n",
    "##############################################################################################            \n",
    "            \n",
    "#find user_size = 5551\n",
    "with open(r\"ctrsr_datasets/citeulike-a/users.dat\") as rating_file:\n",
    "    user_size = len(rating_file.readlines())\n",
    "\n",
    "#initialize rating_matrix (5551 , 16980)\n",
    "import numpy as np\n",
    "rating_matrix = np.zeros((user_size , item_size))\n",
    "\n",
    "#build rating_matrix\n",
    "with open(r\"ctrsr_datasets/citeulike-a/users.dat\") as rating_file:\n",
    "    lines = rating_file.readlines()\n",
    "    for index,line in enumerate(lines):\n",
    "        items = line.strip().split(\" \")\n",
    "        for item in items:  \n",
    "            rating_matrix[index][int(item)] = 1\n",
    "\n",
    "##############################################################################################                        \n",
    "            \n",
    "with open(r'item_infomation_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(item_infomation_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(r'rating_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(rating_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'item_infomation_matrix.pickle', 'rb') as handle:\n",
    "    item_infomation_matrix = pickle.load(handle)  \n",
    "    \n",
    "with open(r'rating_matrix.pickle', 'rb') as handle2:\n",
    "    rating_matrix = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(corruption_level ,size):\n",
    "    mask = np.random.binomial(1, 1 - corruption_level, [size[0],size[1]])\n",
    "    return mask\n",
    "\n",
    "def add_noise(x , corruption_level ):\n",
    "    x = x * mask(corruption_level , x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDL():\n",
    "    def __init__(self , rating_matrix , item_infomation_matrix):\n",
    "        \n",
    "        # model參數設定\n",
    "        self.n_input = 8000\n",
    "        self.n_hidden1 = 200\n",
    "        self.n_hidden2 = 50\n",
    "        self.k = 50\n",
    "        \n",
    "        self.lambda_w = 0.1\n",
    "        self.lambda_n = 10\n",
    "        self.lambda_u = 1\n",
    "        self.lambda_v = 10\n",
    "        \n",
    "        self.drop_ratio = 0.1\n",
    "        self.learning_rate = 0.01\n",
    "        self.epochs = 200\n",
    "        self.batch_size = 256\n",
    "        \n",
    "        self.a = 1\n",
    "        self.b =0.01\n",
    "        self.P = 10\n",
    "        \n",
    "        self.num_u = rating_matrix.shape[0]\n",
    "        self.num_v = rating_matrix.shape[1]\n",
    "        \n",
    "        self.Weights = {\n",
    "            'w1' : tf.Variable(tf.truncated_normal( [self.n_input , self.n_hidden1] , mean=0.0, stddev= tf.truediv(1.0,self.lambda_w))),\n",
    "            'w2' : tf.Variable(tf.truncated_normal( [self.n_hidden1 , self.n_hidden2] , mean=0.0, stddev= tf.truediv(1.0,self.lambda_w))),\n",
    "            'w3' : tf.Variable(tf.truncated_normal( [self.n_hidden2 , self.n_hidden1] , mean=0.0, stddev= tf.truediv(1.0,self.lambda_w))),\n",
    "            'w4' : tf.Variable(tf.truncated_normal( [self.n_hidden1 , self.n_input] , mean=0.0,  stddev= tf.truediv(1.0,self.lambda_w)))   \n",
    "        }\n",
    "        self.Biases = {\n",
    "            'b1' : tf.Variable( tf.zeros(shape=self.n_hidden1) ),\n",
    "            'b2' : tf.Variable( tf.zeros(shape=self.n_hidden2) ),\n",
    "            'b3' : tf.Variable( tf.zeros(shape=self.n_hidden1) ),\n",
    "            'b4' : tf.Variable( tf.zeros(shape=self.n_input) ),\n",
    "        }\n",
    "        \n",
    "        self.item_infomation_matrix = item_infomation_matrix\n",
    "        \n",
    "        self.rating_matrix = rating_matrix\n",
    "        \n",
    "        for i in range(self.num_u):\n",
    "            x = np.random.choice(np.where(self.rating_matrix[i,:]>0)[0] , self.P)\n",
    "            self.rating_matrix[i,:].fill(0)\n",
    "            self.rating_matrix[i,x] = 1\n",
    "        \n",
    "        self.confidence = np.mat(np.ones(self.rating_matrix.shape)) * self.b\n",
    "        self.confidence[np.where(self.rating_matrix>0)] = self.a\n",
    "        \n",
    "    def encoder(self , x , drop_ratio):\n",
    "        w1 = self.Weights['w1']\n",
    "        b1 = self.Biases['b1']\n",
    "        L1 = tf.nn.relu( tf.matmul(x,w1) + b1 )\n",
    "        L1 = tf.nn.dropout( L1 , keep_prob= 1 - drop_ratio )\n",
    "        \n",
    "        w2 = self.Weights['w2']\n",
    "        b2 = self.Biases['b2']\n",
    "        L2 = tf.nn.sigmoid( tf.matmul(L1,w2) + b2 )\n",
    "        L2 = tf.nn.dropout(L2 , keep_prob= 1 - drop_ratio)\n",
    "        \n",
    "        return L2\n",
    "    \n",
    "    def decoder(self , x , drop_ratio):\n",
    "        w3 = self.Weights['w3']\n",
    "        b3 = self.Biases['b3']\n",
    "        L3 = tf.nn.relu(tf.matmul(x,w3) + b3)\n",
    "        L3 = tf.nn.dropout(L3 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        w4 = self.Weights['w4']\n",
    "        b4 = self.Biases['b4']\n",
    "        L4 = tf.nn.sigmoid(tf.matmul(L3,w4) + b4)\n",
    "        L4 = tf.nn.dropout(L4 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        return L4\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        self.X_0 = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.X_c = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.C = tf.placeholder(tf.float32 , shape=(self.num_u,None) )\n",
    "        self.R = tf.placeholder(tf.float32 , shape=(self.num_u,None) )\n",
    "        self.drop_ratio = tf.placeholder(tf.float32)\n",
    "        self.model_batch_data_idx = tf.placeholder( tf.int32 , shape=None )\n",
    "        #SDAE item factor\n",
    "        V_sdae = self.encoder( self.X_0 , self.drop_ratio )\n",
    "        \n",
    "        #SDAE output \n",
    "        sdae_output = self.decoder( V_sdae , self.drop_ratio )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        batch_size = tf.cast(tf.shape(self.X_0)[0], tf.int32)\n",
    "        \n",
    "        \n",
    "        self.V = tf.Variable( tf.zeros(shape=[self.num_v, self.k], dtype=tf.float32 ) ) \n",
    "        self.U = tf.Variable( tf.zeros(shape=[self.num_u, self.k], dtype=tf.float32 ) )\n",
    "        \n",
    "        batch_V = tf.reshape(tf.gather(self.V, self.model_batch_data_idx), shape=[batch_size, self.k])\n",
    "        \n",
    "        loss_1 = self.lambda_u * tf.nn.l2_loss( self.U ) \n",
    "        loss_2 = self.lambda_w * 1/2 * tf.reduce_sum([tf.nn.l2_loss(w)+tf.nn.l2_loss(b) for w,b in zip(self.Weights.values() , self.Biases.values())])\n",
    "        loss_3 = self.lambda_v * tf.nn.l2_loss(batch_V - V_sdae)\n",
    "        loss_4 = self.lambda_n * tf.nn.l2_loss(sdae_output - self.X_c)\n",
    "        \n",
    "        loss_5 = tf.reduce_sum(tf.multiply(self.C ,\n",
    "                                    tf.square(self.R - tf.matmul(self.U , batch_V , transpose_b=True))) \n",
    "                                )\n",
    "        \n",
    "        self.loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "    def train_model(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        random_idx = np.random.permutation(self.num_v)\n",
    "        \n",
    "        self.item_infomation_matrix_noise = add_noise(self.item_infomation_matrix , 0.3)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            batch_cost = 0\n",
    "            for i in range(0 , self.item_infomation_matrix.shape[0] , self.batch_size):\n",
    "                \n",
    "                batch_idx = random_idx[i:i+self.batch_size]\n",
    "                _ , loss = self.sess.run([self.optimizer, self.loss] , \n",
    "                                            feed_dict={self.X_0 : self.item_infomation_matrix_noise[batch_idx,:] , \n",
    "                                                       self.X_c : self.item_infomation_matrix[batch_idx,:] , \n",
    "                                                       self.R : self.rating_matrix[: , batch_idx], \n",
    "                                                       self.C : self.confidence[: , batch_idx], \n",
    "                                                       self.drop_ratio : 0.1 ,\n",
    "                                                       self.model_batch_data_idx  : batch_idx })\n",
    "                batch_cost = batch_cost + loss\n",
    "\n",
    "            print (\"Training //\", \"Epoch %d //\" % (epoch+1), \" Total cost = {:.2f}\".format(batch_cost), \"Elapsed time : %d sec\" % (time.time() - start_time))\n",
    "            \n",
    "        return self.sess.run((tf.matmul(self.U, self.V, transpose_b=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training // Epoch 1 //  Total cost = 773698370.50 Elapsed time : 15 sec\n",
      "Training // Epoch 2 //  Total cost = 705635622.50 Elapsed time : 24 sec\n",
      "Training // Epoch 3 //  Total cost = 645030760.50 Elapsed time : 32 sec\n",
      "Training // Epoch 4 //  Total cost = 587002859.00 Elapsed time : 42 sec\n",
      "Training // Epoch 5 //  Total cost = 532983486.00 Elapsed time : 51 sec\n",
      "Training // Epoch 6 //  Total cost = 485729803.50 Elapsed time : 62 sec\n",
      "Training // Epoch 7 //  Total cost = 446024320.50 Elapsed time : 71 sec\n",
      "Training // Epoch 8 //  Total cost = 415188733.50 Elapsed time : 80 sec\n",
      "Training // Epoch 9 //  Total cost = 392398201.50 Elapsed time : 89 sec\n",
      "Training // Epoch 10 //  Total cost = 375857946.00 Elapsed time : 99 sec\n",
      "Training // Epoch 11 //  Total cost = 363782685.50 Elapsed time : 110 sec\n",
      "Training // Epoch 12 //  Total cost = 354521016.00 Elapsed time : 121 sec\n",
      "Training // Epoch 13 //  Total cost = 347155085.50 Elapsed time : 131 sec\n",
      "Training // Epoch 14 //  Total cost = 340949576.50 Elapsed time : 139 sec\n",
      "Training // Epoch 15 //  Total cost = 335587497.50 Elapsed time : 149 sec\n",
      "Training // Epoch 16 //  Total cost = 330698259.00 Elapsed time : 158 sec\n",
      "Training // Epoch 17 //  Total cost = 326129689.00 Elapsed time : 167 sec\n",
      "Training // Epoch 18 //  Total cost = 321882380.00 Elapsed time : 178 sec\n",
      "Training // Epoch 19 //  Total cost = 317916358.50 Elapsed time : 188 sec\n",
      "Training // Epoch 20 //  Total cost = 314068110.50 Elapsed time : 197 sec\n",
      "Training // Epoch 21 //  Total cost = 310425294.00 Elapsed time : 207 sec\n",
      "Training // Epoch 22 //  Total cost = 306707531.50 Elapsed time : 216 sec\n",
      "Training // Epoch 23 //  Total cost = 303109855.50 Elapsed time : 225 sec\n",
      "Training // Epoch 24 //  Total cost = 299737828.25 Elapsed time : 237 sec\n",
      "Training // Epoch 25 //  Total cost = 296505073.25 Elapsed time : 247 sec\n",
      "Training // Epoch 26 //  Total cost = 293277617.50 Elapsed time : 259 sec\n",
      "Training // Epoch 27 //  Total cost = 290011261.50 Elapsed time : 269 sec\n",
      "Training // Epoch 28 //  Total cost = 286757978.50 Elapsed time : 280 sec\n",
      "Training // Epoch 29 //  Total cost = 283539979.75 Elapsed time : 288 sec\n",
      "Training // Epoch 30 //  Total cost = 280535624.50 Elapsed time : 298 sec\n",
      "Training // Epoch 31 //  Total cost = 277498255.25 Elapsed time : 311 sec\n",
      "Training // Epoch 32 //  Total cost = 274489061.50 Elapsed time : 324 sec\n",
      "Training // Epoch 33 //  Total cost = 271537210.00 Elapsed time : 334 sec\n",
      "Training // Epoch 34 //  Total cost = 268637046.25 Elapsed time : 343 sec\n",
      "Training // Epoch 35 //  Total cost = 265850597.25 Elapsed time : 351 sec\n",
      "Training // Epoch 36 //  Total cost = 262911841.75 Elapsed time : 362 sec\n",
      "Training // Epoch 37 //  Total cost = 259998930.25 Elapsed time : 373 sec\n",
      "Training // Epoch 38 //  Total cost = 257143086.25 Elapsed time : 383 sec\n",
      "Training // Epoch 39 //  Total cost = 254344303.00 Elapsed time : 393 sec\n",
      "Training // Epoch 40 //  Total cost = 251651700.00 Elapsed time : 402 sec\n",
      "Training // Epoch 41 //  Total cost = 248941759.50 Elapsed time : 411 sec\n",
      "Training // Epoch 42 //  Total cost = 246148671.50 Elapsed time : 424 sec\n",
      "Training // Epoch 43 //  Total cost = 243345429.50 Elapsed time : 433 sec\n",
      "Training // Epoch 44 //  Total cost = 240593920.75 Elapsed time : 442 sec\n",
      "Training // Epoch 45 //  Total cost = 237880507.75 Elapsed time : 450 sec\n",
      "Training // Epoch 46 //  Total cost = 235275561.75 Elapsed time : 462 sec\n",
      "Training // Epoch 47 //  Total cost = 232494662.50 Elapsed time : 471 sec\n",
      "Training // Epoch 48 //  Total cost = 229787571.25 Elapsed time : 480 sec\n",
      "Training // Epoch 49 //  Total cost = 227156077.75 Elapsed time : 490 sec\n",
      "Training // Epoch 50 //  Total cost = 224512525.50 Elapsed time : 498 sec\n",
      "Training // Epoch 51 //  Total cost = 221911289.75 Elapsed time : 508 sec\n",
      "Training // Epoch 52 //  Total cost = 219268600.50 Elapsed time : 517 sec\n",
      "Training // Epoch 53 //  Total cost = 216633722.50 Elapsed time : 525 sec\n",
      "Training // Epoch 54 //  Total cost = 214175008.50 Elapsed time : 535 sec\n",
      "Training // Epoch 55 //  Total cost = 211558320.25 Elapsed time : 545 sec\n",
      "Training // Epoch 56 //  Total cost = 209100491.25 Elapsed time : 553 sec\n",
      "Training // Epoch 57 //  Total cost = 206689012.00 Elapsed time : 563 sec\n",
      "Training // Epoch 58 //  Total cost = 204287685.25 Elapsed time : 571 sec\n",
      "Training // Epoch 59 //  Total cost = 201901333.00 Elapsed time : 580 sec\n",
      "Training // Epoch 60 //  Total cost = 199549609.00 Elapsed time : 589 sec\n",
      "Training // Epoch 61 //  Total cost = 197203979.25 Elapsed time : 599 sec\n",
      "Training // Epoch 62 //  Total cost = 194866842.25 Elapsed time : 608 sec\n",
      "Training // Epoch 63 //  Total cost = 192637053.50 Elapsed time : 617 sec\n",
      "Training // Epoch 64 //  Total cost = 190441433.75 Elapsed time : 627 sec\n",
      "Training // Epoch 65 //  Total cost = 188121022.00 Elapsed time : 636 sec\n",
      "Training // Epoch 66 //  Total cost = 185872605.25 Elapsed time : 646 sec\n",
      "Training // Epoch 67 //  Total cost = 183715585.25 Elapsed time : 655 sec\n",
      "Training // Epoch 68 //  Total cost = 181536329.25 Elapsed time : 665 sec\n",
      "Training // Epoch 69 //  Total cost = 179352512.00 Elapsed time : 673 sec\n",
      "Training // Epoch 70 //  Total cost = 177190037.00 Elapsed time : 682 sec\n",
      "Training // Epoch 71 //  Total cost = 174878427.50 Elapsed time : 691 sec\n",
      "Training // Epoch 72 //  Total cost = 172883350.25 Elapsed time : 700 sec\n",
      "Training // Epoch 73 //  Total cost = 170755656.75 Elapsed time : 709 sec\n",
      "Training // Epoch 74 //  Total cost = 168657484.25 Elapsed time : 718 sec\n",
      "Training // Epoch 75 //  Total cost = 166604657.25 Elapsed time : 727 sec\n",
      "Training // Epoch 76 //  Total cost = 164680221.25 Elapsed time : 736 sec\n",
      "Training // Epoch 77 //  Total cost = 162729807.75 Elapsed time : 746 sec\n",
      "Training // Epoch 78 //  Total cost = 160648662.75 Elapsed time : 755 sec\n",
      "Training // Epoch 79 //  Total cost = 158738196.00 Elapsed time : 764 sec\n",
      "Training // Epoch 80 //  Total cost = 156833284.50 Elapsed time : 773 sec\n",
      "Training // Epoch 81 //  Total cost = 154900159.88 Elapsed time : 783 sec\n",
      "Training // Epoch 82 //  Total cost = 152997824.12 Elapsed time : 794 sec\n",
      "Training // Epoch 83 //  Total cost = 151107263.12 Elapsed time : 804 sec\n",
      "Training // Epoch 84 //  Total cost = 149251767.50 Elapsed time : 814 sec\n",
      "Training // Epoch 85 //  Total cost = 147410853.75 Elapsed time : 824 sec\n",
      "Training // Epoch 86 //  Total cost = 145572707.75 Elapsed time : 833 sec\n",
      "Training // Epoch 87 //  Total cost = 143711883.75 Elapsed time : 842 sec\n",
      "Training // Epoch 88 //  Total cost = 141931121.88 Elapsed time : 851 sec\n",
      "Training // Epoch 89 //  Total cost = 140180611.75 Elapsed time : 860 sec\n",
      "Training // Epoch 90 //  Total cost = 138363242.38 Elapsed time : 868 sec\n",
      "Training // Epoch 91 //  Total cost = 136651993.75 Elapsed time : 876 sec\n",
      "Training // Epoch 92 //  Total cost = 134958567.38 Elapsed time : 885 sec\n",
      "Training // Epoch 93 //  Total cost = 133290614.12 Elapsed time : 895 sec\n",
      "Training // Epoch 94 //  Total cost = 131639836.38 Elapsed time : 904 sec\n",
      "Training // Epoch 95 //  Total cost = 130010798.62 Elapsed time : 913 sec\n",
      "Training // Epoch 96 //  Total cost = 128395054.00 Elapsed time : 921 sec\n",
      "Training // Epoch 97 //  Total cost = 126800578.50 Elapsed time : 932 sec\n",
      "Training // Epoch 98 //  Total cost = 125229816.88 Elapsed time : 942 sec\n",
      "Training // Epoch 99 //  Total cost = 123692435.12 Elapsed time : 951 sec\n",
      "Training // Epoch 100 //  Total cost = 122237442.75 Elapsed time : 961 sec\n",
      "Training // Epoch 101 //  Total cost = 120725213.25 Elapsed time : 971 sec\n",
      "Training // Epoch 102 //  Total cost = 119234900.12 Elapsed time : 980 sec\n",
      "Training // Epoch 103 //  Total cost = 117874661.62 Elapsed time : 988 sec\n",
      "Training // Epoch 104 //  Total cost = 116470678.88 Elapsed time : 997 sec\n",
      "Training // Epoch 105 //  Total cost = 115022655.12 Elapsed time : 1006 sec\n",
      "Training // Epoch 106 //  Total cost = 113581533.00 Elapsed time : 1015 sec\n",
      "Training // Epoch 107 //  Total cost = 112063482.25 Elapsed time : 1026 sec\n",
      "Training // Epoch 108 //  Total cost = 110534849.00 Elapsed time : 1034 sec\n",
      "Training // Epoch 109 //  Total cost = 109188508.50 Elapsed time : 1045 sec\n",
      "Training // Epoch 110 //  Total cost = 107850101.00 Elapsed time : 1054 sec\n",
      "Training // Epoch 111 //  Total cost = 106489464.25 Elapsed time : 1064 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training // Epoch 112 //  Total cost = 105149872.12 Elapsed time : 1073 sec\n",
      "Training // Epoch 113 //  Total cost = 103831329.12 Elapsed time : 1082 sec\n",
      "Training // Epoch 114 //  Total cost = 102520898.50 Elapsed time : 1091 sec\n",
      "Training // Epoch 115 //  Total cost = 101217275.62 Elapsed time : 1099 sec\n",
      "Training // Epoch 116 //  Total cost = 99933207.00 Elapsed time : 1108 sec\n",
      "Training // Epoch 117 //  Total cost = 98657446.38 Elapsed time : 1117 sec\n",
      "Training // Epoch 118 //  Total cost = 97319366.88 Elapsed time : 1126 sec\n",
      "Training // Epoch 119 //  Total cost = 95963378.25 Elapsed time : 1135 sec\n",
      "Training // Epoch 120 //  Total cost = 94728445.25 Elapsed time : 1146 sec\n",
      "Training // Epoch 121 //  Total cost = 93502661.25 Elapsed time : 1156 sec\n",
      "Training // Epoch 122 //  Total cost = 92330108.62 Elapsed time : 1165 sec\n",
      "Training // Epoch 123 //  Total cost = 91174529.12 Elapsed time : 1173 sec\n",
      "Training // Epoch 124 //  Total cost = 89978735.50 Elapsed time : 1182 sec\n",
      "Training // Epoch 125 //  Total cost = 88797236.50 Elapsed time : 1191 sec\n",
      "Training // Epoch 126 //  Total cost = 87537969.75 Elapsed time : 1201 sec\n",
      "Training // Epoch 127 //  Total cost = 86369391.06 Elapsed time : 1211 sec\n",
      "Training // Epoch 128 //  Total cost = 85232164.44 Elapsed time : 1219 sec\n",
      "Training // Epoch 129 //  Total cost = 84079958.69 Elapsed time : 1228 sec\n",
      "Training // Epoch 130 //  Total cost = 82958616.38 Elapsed time : 1239 sec\n",
      "Training // Epoch 131 //  Total cost = 81832741.69 Elapsed time : 1247 sec\n",
      "Training // Epoch 132 //  Total cost = 80730423.06 Elapsed time : 1256 sec\n",
      "Training // Epoch 133 //  Total cost = 79635883.12 Elapsed time : 1266 sec\n",
      "Training // Epoch 134 //  Total cost = 78551486.81 Elapsed time : 1275 sec\n",
      "Training // Epoch 135 //  Total cost = 77488235.56 Elapsed time : 1284 sec\n",
      "Training // Epoch 136 //  Total cost = 76431740.50 Elapsed time : 1293 sec\n",
      "Training // Epoch 137 //  Total cost = 75378988.94 Elapsed time : 1302 sec\n",
      "Training // Epoch 138 //  Total cost = 74341346.75 Elapsed time : 1310 sec\n",
      "Training // Epoch 139 //  Total cost = 73325232.38 Elapsed time : 1320 sec\n",
      "Training // Epoch 140 //  Total cost = 72296554.75 Elapsed time : 1329 sec\n",
      "Training // Epoch 141 //  Total cost = 71296145.62 Elapsed time : 1338 sec\n",
      "Training // Epoch 142 //  Total cost = 70338238.56 Elapsed time : 1347 sec\n",
      "Training // Epoch 143 //  Total cost = 69525732.75 Elapsed time : 1357 sec\n",
      "Training // Epoch 144 //  Total cost = 68608610.31 Elapsed time : 1366 sec\n",
      "Training // Epoch 145 //  Total cost = 67763551.38 Elapsed time : 1375 sec\n",
      "Training // Epoch 146 //  Total cost = 66916529.81 Elapsed time : 1384 sec\n",
      "Training // Epoch 147 //  Total cost = 66104130.81 Elapsed time : 1394 sec\n",
      "Training // Epoch 148 //  Total cost = 65184328.94 Elapsed time : 1403 sec\n",
      "Training // Epoch 149 //  Total cost = 64259965.88 Elapsed time : 1412 sec\n",
      "Training // Epoch 150 //  Total cost = 63347421.81 Elapsed time : 1421 sec\n",
      "Training // Epoch 151 //  Total cost = 62454146.12 Elapsed time : 1430 sec\n",
      "Training // Epoch 152 //  Total cost = 61651348.75 Elapsed time : 1440 sec\n",
      "Training // Epoch 153 //  Total cost = 60882634.38 Elapsed time : 1448 sec\n",
      "Training // Epoch 154 //  Total cost = 60006371.50 Elapsed time : 1458 sec\n",
      "Training // Epoch 155 //  Total cost = 59158085.00 Elapsed time : 1466 sec\n",
      "Training // Epoch 156 //  Total cost = 58228145.31 Elapsed time : 1475 sec\n",
      "Training // Epoch 157 //  Total cost = 57402178.75 Elapsed time : 1485 sec\n",
      "Training // Epoch 158 //  Total cost = 56687078.25 Elapsed time : 1494 sec\n",
      "Training // Epoch 159 //  Total cost = 55934351.38 Elapsed time : 1503 sec\n",
      "Training // Epoch 160 //  Total cost = 55196656.19 Elapsed time : 1513 sec\n",
      "Training // Epoch 161 //  Total cost = 54526469.19 Elapsed time : 1522 sec\n",
      "Training // Epoch 162 //  Total cost = 53794929.38 Elapsed time : 1532 sec\n",
      "Training // Epoch 163 //  Total cost = 53227911.06 Elapsed time : 1541 sec\n",
      "Training // Epoch 164 //  Total cost = 52495633.03 Elapsed time : 1550 sec\n",
      "Training // Epoch 165 //  Total cost = 51686031.72 Elapsed time : 1559 sec\n",
      "Training // Epoch 166 //  Total cost = 51065743.72 Elapsed time : 1568 sec\n",
      "Training // Epoch 167 //  Total cost = 50613691.03 Elapsed time : 1578 sec\n",
      "Training // Epoch 168 //  Total cost = 50010391.62 Elapsed time : 1586 sec\n",
      "Training // Epoch 169 //  Total cost = 49251662.28 Elapsed time : 1595 sec\n",
      "Training // Epoch 170 //  Total cost = 48476573.28 Elapsed time : 1605 sec\n",
      "Training // Epoch 171 //  Total cost = 47770026.00 Elapsed time : 1614 sec\n",
      "Training // Epoch 172 //  Total cost = 47241335.53 Elapsed time : 1623 sec\n",
      "Training // Epoch 173 //  Total cost = 46681838.00 Elapsed time : 1632 sec\n",
      "Training // Epoch 174 //  Total cost = 46317956.78 Elapsed time : 1642 sec\n",
      "Training // Epoch 175 //  Total cost = 45943084.44 Elapsed time : 1651 sec\n",
      "Training // Epoch 176 //  Total cost = 45386579.72 Elapsed time : 1660 sec\n",
      "Training // Epoch 177 //  Total cost = 44922738.41 Elapsed time : 1669 sec\n",
      "Training // Epoch 178 //  Total cost = 44322517.16 Elapsed time : 1678 sec\n",
      "Training // Epoch 179 //  Total cost = 43986521.75 Elapsed time : 1687 sec\n",
      "Training // Epoch 180 //  Total cost = 43758601.81 Elapsed time : 1696 sec\n",
      "Training // Epoch 181 //  Total cost = 43722881.03 Elapsed time : 1705 sec\n",
      "Training // Epoch 182 //  Total cost = 43476496.03 Elapsed time : 1715 sec\n",
      "Training // Epoch 183 //  Total cost = 43183471.28 Elapsed time : 1725 sec\n",
      "Training // Epoch 184 //  Total cost = 43066373.31 Elapsed time : 1733 sec\n",
      "Training // Epoch 185 //  Total cost = 42607169.78 Elapsed time : 1742 sec\n",
      "Training // Epoch 186 //  Total cost = 42166879.16 Elapsed time : 1752 sec\n",
      "Training // Epoch 187 //  Total cost = 42024122.44 Elapsed time : 1761 sec\n",
      "Training // Epoch 188 //  Total cost = 42372008.22 Elapsed time : 1772 sec\n",
      "Training // Epoch 189 //  Total cost = 42138658.41 Elapsed time : 1781 sec\n",
      "Training // Epoch 190 //  Total cost = 42213289.88 Elapsed time : 1791 sec\n",
      "Training // Epoch 191 //  Total cost = 41756781.59 Elapsed time : 1801 sec\n",
      "Training // Epoch 192 //  Total cost = 41396363.56 Elapsed time : 1810 sec\n",
      "Training // Epoch 193 //  Total cost = 41584611.12 Elapsed time : 1821 sec\n",
      "Training // Epoch 194 //  Total cost = 41342363.47 Elapsed time : 1830 sec\n",
      "Training // Epoch 195 //  Total cost = 40909501.94 Elapsed time : 1840 sec\n",
      "Training // Epoch 196 //  Total cost = 40905762.28 Elapsed time : 1849 sec\n",
      "Training // Epoch 197 //  Total cost = 40677539.88 Elapsed time : 1858 sec\n",
      "Training // Epoch 198 //  Total cost = 40143575.75 Elapsed time : 1867 sec\n",
      "Training // Epoch 199 //  Total cost = 40041040.94 Elapsed time : 1877 sec\n",
      "Training // Epoch 200 //  Total cost = 40477187.25 Elapsed time : 1888 sec\n"
     ]
    }
   ],
   "source": [
    "R_train = rating_matrix.copy()\n",
    "cdl = CDL(R_train , item_infomation_matrix)\n",
    "cdl.build_model()\n",
    "R = cdl.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnt = 0\n",
    "for i in range(rating_matrix.shape[0]):\n",
    "    l_score = np.ravel(R[i,:]).tolist()\n",
    "    pl = sorted(enumerate(l_score),key=lambda d:d[1],reverse=True)\n",
    "    l_rec = [i[0] for i in pl][:300]\n",
    "    s_rec = set(l_rec)\n",
    "    s_true = set(np.ravel(np.where(rating_matrix[i,:]>0)))\n",
    "    cnt_hit = len(s_rec.intersection(s_true))\n",
    "    all_cnt = all_cnt + cnt_hit/len(s_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20727739043650245\n"
     ]
    }
   ],
   "source": [
    "print(all_cnt/rating_matrix.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
