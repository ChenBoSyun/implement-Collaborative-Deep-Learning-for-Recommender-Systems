{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDL by tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k12s35h813g\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "#init random seed\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build item information matrix of citeulike-a by bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find vocabulary_size = 8000\n",
    "with open(r\"ctrsr_datasets/citeulike-a/vocabulary.dat\") as vocabulary_file:\n",
    "    vocabulary_size = len(vocabulary_file.readlines())\n",
    "    \n",
    "#find item_size = 16980\n",
    "with open(r\"ctrsr_datasets/citeulike-a/mult.dat\") as item_info_file:\n",
    "    item_size = len(item_info_file.readlines())\n",
    "\n",
    "#initialize item_infomation_matrix (16980 , 8000)\n",
    "item_infomation_matrix = np.zeros((item_size , vocabulary_size))\n",
    "\n",
    "#build item_infomation_matrix\n",
    "with open(r\"ctrsr_datasets/citeulike-a/mult.dat\") as item_info_file:\n",
    "    sentences = item_info_file.readlines()\n",
    "    \n",
    "    for index,sentence in enumerate(sentences):\n",
    "        words = sentence.strip().split(\" \")[1:]\n",
    "        for word in words:\n",
    "            vocabulary_index , number = word.split(\":\")\n",
    "            item_infomation_matrix[index][int(vocabulary_index)] =number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build rating matrix citeulike-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find user_size = 5551\n",
    "with open(r\"ctrsr_datasets/citeulike-a/users.dat\") as rating_file:\n",
    "    user_size = len(rating_file.readlines())\n",
    "\n",
    "#initialize rating_matrix (5551 , 16980)\n",
    "import numpy as np\n",
    "rating_matrix = np.zeros((user_size , item_size))\n",
    "\n",
    "#build rating_matrix\n",
    "with open(r\"ctrsr_datasets/citeulike-a/users.dat\") as rating_file:\n",
    "    lines = rating_file.readlines()\n",
    "    for index,line in enumerate(lines):\n",
    "        items = line.strip().split(\" \")\n",
    "        for item in items:  \n",
    "            rating_matrix[index][int(item)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save matrix by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'item_infomation_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(item_infomation_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(r'rating_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(rating_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load matrix from pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'item_infomation_matrix.pickle', 'rb') as handle:\n",
    "    item_infomation_matrix = pickle.load(handle)  \n",
    "    \n",
    "with open(r'rating_matrix.pickle', 'rb') as handle2:\n",
    "    rating_matrix = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply SDAE : we hope to reconstruct item information by masking nosie\n",
    "def mask(corruption_level ,size):\n",
    "    mask = np.random.binomial(1, 1 - corruption_level, [size[0],size[1]])\n",
    "    return mask\n",
    "\n",
    "def add_noise(x , corruption_level ):\n",
    "    x = x * mask(corruption_level , x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDL():\n",
    "    def __init__(self , rating_matrix , item_infomation_matrix):\n",
    "        \n",
    "        # model參數設定\n",
    "        self.n_input = item_infomation_matrix.shape[1]\n",
    "        self.n_hidden1 = 200\n",
    "        self.n_hidden2 = 50\n",
    "        self.k = 50\n",
    "        \n",
    "        self.lambda_w = 0.1\n",
    "        self.lambda_n = 10\n",
    "        self.lambda_u = 1\n",
    "        self.lambda_v = 10\n",
    "        \n",
    "        self.drop_ratio = 0.1\n",
    "        self.learning_rate = 0.01\n",
    "        self.epochs = 200\n",
    "        self.batch_size = 256\n",
    "        \n",
    "        self.a = 1\n",
    "        self.b =0.01\n",
    "        self.P = 1\n",
    "        \n",
    "        self.num_u = rating_matrix.shape[0]\n",
    "        self.num_v = rating_matrix.shape[1]\n",
    "        \n",
    "        self.Weights = {\n",
    "            'w1' : tf.Variable(tf.truncated_normal( [self.n_input , self.n_hidden1] , mean=0.0, stddev= tf.truediv(1.0,self.lambda_w))),\n",
    "            'w2' : tf.Variable(tf.truncated_normal( [self.n_hidden1 , self.n_hidden2] , mean=0.0, stddev= tf.truediv(1.0,self.lambda_w))),\n",
    "            'w3' : tf.Variable(tf.truncated_normal( [self.n_hidden2 , self.n_hidden1] , mean=0.0, stddev= tf.truediv(1.0,self.lambda_w))),\n",
    "            'w4' : tf.Variable(tf.truncated_normal( [self.n_hidden1 , self.n_input] , mean=0.0,  stddev= tf.truediv(1.0,self.lambda_w)))   \n",
    "        }\n",
    "        self.Biases = {\n",
    "            'b1' : tf.Variable( tf.zeros(shape=self.n_hidden1) ),\n",
    "            'b2' : tf.Variable( tf.zeros(shape=self.n_hidden2) ),\n",
    "            'b3' : tf.Variable( tf.zeros(shape=self.n_hidden1) ),\n",
    "            'b4' : tf.Variable( tf.zeros(shape=self.n_input) ),\n",
    "        }\n",
    "        \n",
    "        self.item_infomation_matrix = item_infomation_matrix\n",
    "        \n",
    "        self.rating_matrix = rating_matrix\n",
    "        \n",
    "        for i in range(self.num_u):\n",
    "            x = np.random.choice(np.where(self.rating_matrix[i,:]>0)[0] , self.P)\n",
    "            self.rating_matrix[i,:].fill(0)\n",
    "            self.rating_matrix[i,x] = 1\n",
    "        \n",
    "        self.confidence = np.mat(np.ones(self.rating_matrix.shape)) * self.b\n",
    "        self.confidence[np.where(self.rating_matrix>0)] = self.a\n",
    "        \n",
    "    def encoder(self , x , drop_ratio):\n",
    "        w1 = self.Weights['w1']\n",
    "        b1 = self.Biases['b1']\n",
    "        L1 = tf.nn.sigmoid( tf.matmul(x,w1) + b1 )\n",
    "        L1 = tf.nn.dropout( L1 , keep_prob= 1 - drop_ratio )\n",
    "        \n",
    "        w2 = self.Weights['w2']\n",
    "        b2 = self.Biases['b2']\n",
    "        L2 = tf.nn.sigmoid( tf.matmul(L1,w2) + b2 )\n",
    "        L2 = tf.nn.dropout(L2 , keep_prob= 1 - drop_ratio)\n",
    "        \n",
    "        return L2\n",
    "    \n",
    "    def decoder(self , x , drop_ratio):\n",
    "        w3 = self.Weights['w3']\n",
    "        b3 = self.Biases['b3']\n",
    "        L3 = tf.nn.sigmoid(tf.matmul(x,w3) + b3)\n",
    "        L3 = tf.nn.dropout(L3 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        w4 = self.Weights['w4']\n",
    "        b4 = self.Biases['b4']\n",
    "        L4 = tf.nn.sigmoid(tf.matmul(L3,w4) + b4)\n",
    "        L4 = tf.nn.dropout(L4 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        return L4\n",
    "    \n",
    "#     def only_MF(self):\n",
    "#         self.C = tf.placeholder(tf.float32 , shape=(self.num_u,None) )\n",
    "#         self.R = tf.placeholder(tf.float32 , shape=(self.num_u,None) )\n",
    "#         self.drop_ratio = tf.placeholder(tf.float32)\n",
    "#         self.model_batch_data_idx = tf.placeholder( tf.int32 , shape=None )\n",
    "        \n",
    "#         batch_size = tf.cast(tf.shape(self.R)[1], tf.int32)\n",
    "        \n",
    "        \n",
    "#         self.V = tf.Variable( tf.zeros(shape=[self.num_v, self.k], dtype=tf.float32 ) ) \n",
    "#         self.U = tf.Variable( tf.zeros(shape=[self.num_u, self.k], dtype=tf.float32 ) )\n",
    "        \n",
    "#         batch_V = tf.reshape(tf.gather(self.V, self.model_batch_data_idx), shape=[batch_size, self.k])\n",
    "        \n",
    "#         loss_1 = self.lambda_u * tf.nn.l2_loss( self.U ) \n",
    "#         loss_2 = tf.reduce_sum(tf.multiply(self.C ,\n",
    "#                                     tf.square(self.R - tf.matmul(self.U , batch_V , transpose_b=True))) \n",
    "#                                 )\n",
    "        \n",
    "#         self.loss = loss_1 + loss_2 \n",
    "#         self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        self.X_0 = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.X_c = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.C = tf.placeholder(tf.float32 , shape=(self.num_u,None) )\n",
    "        self.R = tf.placeholder(tf.float32 , shape=(self.num_u,None) )\n",
    "        self.drop_ratio = tf.placeholder(tf.float32)\n",
    "        self.model_batch_data_idx = tf.placeholder( tf.int32 , shape=None )\n",
    "        #SDAE item factor\n",
    "        V_sdae = self.encoder( self.X_0 , self.drop_ratio )\n",
    "        \n",
    "        #SDAE output \n",
    "        sdae_output = self.decoder( V_sdae , self.drop_ratio )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        batch_size = tf.cast(tf.shape(self.X_0)[0], tf.int32)\n",
    "        \n",
    "        \n",
    "        self.V = tf.Variable( tf.zeros(shape=[self.num_v, self.k], dtype=tf.float32 ) ) \n",
    "        self.U = tf.Variable( tf.zeros(shape=[self.num_u, self.k], dtype=tf.float32 ) )\n",
    "        \n",
    "        batch_V = tf.reshape(tf.gather(self.V, self.model_batch_data_idx), shape=[batch_size, self.k])\n",
    "        \n",
    "        loss_1 = self.lambda_u * tf.nn.l2_loss( self.U ) \n",
    "        loss_2 = self.lambda_w * 1/2 * tf.reduce_sum([tf.nn.l2_loss(w)+tf.nn.l2_loss(b) for w,b in zip(self.Weights.values() , self.Biases.values())])\n",
    "        loss_3 = self.lambda_v * tf.nn.l2_loss(batch_V - V_sdae)\n",
    "        loss_4 = self.lambda_n * tf.nn.l2_loss(sdae_output - self.X_c)\n",
    "        \n",
    "        loss_5 = tf.reduce_sum(tf.multiply(self.C ,\n",
    "                                    tf.square(self.R - tf.matmul(self.U , batch_V , transpose_b=True))) \n",
    "                                )\n",
    "        \n",
    "        self.loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "    def train_model(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        random_idx = np.random.permutation(self.num_v)\n",
    "        \n",
    "        self.item_infomation_matrix_noise = add_noise(self.item_infomation_matrix , 0.3)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            batch_cost = 0\n",
    "            for i in range(0 , self.item_infomation_matrix.shape[0] , self.batch_size):\n",
    "                \n",
    "                batch_idx = random_idx[i:i+self.batch_size]\n",
    "                _ , loss = self.sess.run([self.optimizer, self.loss] , \n",
    "                                            feed_dict={self.X_0 : self.item_infomation_matrix_noise[batch_idx,:] , \n",
    "                                                       self.X_c : self.item_infomation_matrix[batch_idx,:] , \n",
    "                                                       self.R : self.rating_matrix[: , batch_idx], \n",
    "                                                       self.C : self.confidence[: , batch_idx], \n",
    "                                                       self.drop_ratio : 0.1 ,\n",
    "                                                       self.model_batch_data_idx  : batch_idx })\n",
    "                batch_cost = batch_cost + loss\n",
    "\n",
    "            print (\"Training //\", \"Epoch %d //\" % (epoch+1), \" Total cost = {:.2f}\".format(batch_cost), \"Elapsed time : %d sec\" % (time.time() - start_time))\n",
    "            \n",
    "        return self.sess.run((tf.matmul(self.U, self.V, transpose_b=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training // Epoch 1 //  Total cost = 699676798.00 Elapsed time : 13 sec\n",
      "Training // Epoch 2 //  Total cost = 509001361.50 Elapsed time : 23 sec\n",
      "Training // Epoch 3 //  Total cost = 426090375.00 Elapsed time : 32 sec\n",
      "Training // Epoch 4 //  Total cost = 402229449.00 Elapsed time : 40 sec\n",
      "Training // Epoch 5 //  Total cost = 385809588.50 Elapsed time : 50 sec\n",
      "Training // Epoch 6 //  Total cost = 370855234.50 Elapsed time : 59 sec\n",
      "Training // Epoch 7 //  Total cost = 356779552.50 Elapsed time : 68 sec\n",
      "Training // Epoch 8 //  Total cost = 343552683.50 Elapsed time : 77 sec\n",
      "Training // Epoch 9 //  Total cost = 330934537.50 Elapsed time : 85 sec\n",
      "Training // Epoch 10 //  Total cost = 318868322.50 Elapsed time : 94 sec\n",
      "Training // Epoch 11 //  Total cost = 307315671.75 Elapsed time : 102 sec\n",
      "Training // Epoch 12 //  Total cost = 296283195.75 Elapsed time : 112 sec\n",
      "Training // Epoch 13 //  Total cost = 285720805.75 Elapsed time : 121 sec\n",
      "Training // Epoch 14 //  Total cost = 275675537.25 Elapsed time : 130 sec\n",
      "Training // Epoch 15 //  Total cost = 265893138.00 Elapsed time : 139 sec\n",
      "Training // Epoch 16 //  Total cost = 256381356.50 Elapsed time : 147 sec\n",
      "Training // Epoch 17 //  Total cost = 247409903.50 Elapsed time : 157 sec\n",
      "Training // Epoch 18 //  Total cost = 238836460.75 Elapsed time : 165 sec\n",
      "Training // Epoch 19 //  Total cost = 230363394.25 Elapsed time : 174 sec\n",
      "Training // Epoch 20 //  Total cost = 222478204.00 Elapsed time : 183 sec\n",
      "Training // Epoch 21 //  Total cost = 214707066.00 Elapsed time : 192 sec\n",
      "Training // Epoch 22 //  Total cost = 207398870.00 Elapsed time : 201 sec\n",
      "Training // Epoch 23 //  Total cost = 200335661.00 Elapsed time : 210 sec\n",
      "Training // Epoch 24 //  Total cost = 193502709.25 Elapsed time : 219 sec\n",
      "Training // Epoch 25 //  Total cost = 187017720.00 Elapsed time : 227 sec\n",
      "Training // Epoch 26 //  Total cost = 180656876.50 Elapsed time : 236 sec\n",
      "Training // Epoch 27 //  Total cost = 174520094.75 Elapsed time : 244 sec\n",
      "Training // Epoch 28 //  Total cost = 168618046.00 Elapsed time : 253 sec\n",
      "Training // Epoch 29 //  Total cost = 162554788.25 Elapsed time : 262 sec\n",
      "Training // Epoch 30 //  Total cost = 157096778.00 Elapsed time : 271 sec\n",
      "Training // Epoch 31 //  Total cost = 151624672.62 Elapsed time : 280 sec\n",
      "Training // Epoch 32 //  Total cost = 146411205.25 Elapsed time : 288 sec\n",
      "Training // Epoch 33 //  Total cost = 141438258.50 Elapsed time : 297 sec\n",
      "Training // Epoch 34 //  Total cost = 136530659.00 Elapsed time : 305 sec\n",
      "Training // Epoch 35 //  Total cost = 131753547.00 Elapsed time : 314 sec\n",
      "Training // Epoch 36 //  Total cost = 126863433.50 Elapsed time : 323 sec\n",
      "Training // Epoch 37 //  Total cost = 122656202.88 Elapsed time : 332 sec\n",
      "Training // Epoch 38 //  Total cost = 118599578.00 Elapsed time : 340 sec\n",
      "Training // Epoch 39 //  Total cost = 114611759.88 Elapsed time : 348 sec\n",
      "Training // Epoch 40 //  Total cost = 110903898.50 Elapsed time : 357 sec\n",
      "Training // Epoch 41 //  Total cost = 107518569.25 Elapsed time : 365 sec\n",
      "Training // Epoch 42 //  Total cost = 104367943.38 Elapsed time : 373 sec\n",
      "Training // Epoch 43 //  Total cost = 101399079.12 Elapsed time : 382 sec\n",
      "Training // Epoch 44 //  Total cost = 98611119.12 Elapsed time : 391 sec\n",
      "Training // Epoch 45 //  Total cost = 95935625.75 Elapsed time : 400 sec\n",
      "Training // Epoch 46 //  Total cost = 93366417.50 Elapsed time : 409 sec\n",
      "Training // Epoch 47 //  Total cost = 90901260.00 Elapsed time : 417 sec\n",
      "Training // Epoch 48 //  Total cost = 88532784.75 Elapsed time : 426 sec\n",
      "Training // Epoch 49 //  Total cost = 86253099.31 Elapsed time : 435 sec\n",
      "Training // Epoch 50 //  Total cost = 84065897.81 Elapsed time : 444 sec\n",
      "Training // Epoch 51 //  Total cost = 81960649.31 Elapsed time : 452 sec\n",
      "Training // Epoch 52 //  Total cost = 79932892.19 Elapsed time : 461 sec\n",
      "Training // Epoch 53 //  Total cost = 77983110.50 Elapsed time : 470 sec\n",
      "Training // Epoch 54 //  Total cost = 76107443.06 Elapsed time : 478 sec\n",
      "Training // Epoch 55 //  Total cost = 74296636.31 Elapsed time : 486 sec\n",
      "Training // Epoch 56 //  Total cost = 72557214.06 Elapsed time : 495 sec\n",
      "Training // Epoch 57 //  Total cost = 70873424.69 Elapsed time : 503 sec\n",
      "Training // Epoch 58 //  Total cost = 69249254.19 Elapsed time : 512 sec\n",
      "Training // Epoch 59 //  Total cost = 67679117.94 Elapsed time : 521 sec\n",
      "Training // Epoch 60 //  Total cost = 66165503.62 Elapsed time : 530 sec\n",
      "Training // Epoch 61 //  Total cost = 64698699.00 Elapsed time : 539 sec\n",
      "Training // Epoch 62 //  Total cost = 63282957.81 Elapsed time : 547 sec\n",
      "Training // Epoch 63 //  Total cost = 61906538.56 Elapsed time : 556 sec\n",
      "Training // Epoch 64 //  Total cost = 60575016.38 Elapsed time : 565 sec\n",
      "Training // Epoch 65 //  Total cost = 59282706.25 Elapsed time : 574 sec\n",
      "Training // Epoch 66 //  Total cost = 58029875.38 Elapsed time : 583 sec\n",
      "Training // Epoch 67 //  Total cost = 56815220.81 Elapsed time : 592 sec\n",
      "Training // Epoch 68 //  Total cost = 55630129.94 Elapsed time : 601 sec\n",
      "Training // Epoch 69 //  Total cost = 54482065.69 Elapsed time : 609 sec\n",
      "Training // Epoch 70 //  Total cost = 53366296.62 Elapsed time : 618 sec\n",
      "Training // Epoch 71 //  Total cost = 52280660.06 Elapsed time : 627 sec\n",
      "Training // Epoch 72 //  Total cost = 51223257.59 Elapsed time : 635 sec\n",
      "Training // Epoch 73 //  Total cost = 50193597.88 Elapsed time : 644 sec\n",
      "Training // Epoch 74 //  Total cost = 49194452.50 Elapsed time : 652 sec\n",
      "Training // Epoch 75 //  Total cost = 48220310.16 Elapsed time : 661 sec\n",
      "Training // Epoch 76 //  Total cost = 47270594.84 Elapsed time : 669 sec\n",
      "Training // Epoch 77 //  Total cost = 46346913.22 Elapsed time : 678 sec\n",
      "Training // Epoch 78 //  Total cost = 45445881.34 Elapsed time : 687 sec\n",
      "Training // Epoch 79 //  Total cost = 44570288.53 Elapsed time : 696 sec\n",
      "Training // Epoch 80 //  Total cost = 43717168.12 Elapsed time : 704 sec\n",
      "Training // Epoch 81 //  Total cost = 42887369.91 Elapsed time : 713 sec\n",
      "Training // Epoch 82 //  Total cost = 42079221.00 Elapsed time : 721 sec\n",
      "Training // Epoch 83 //  Total cost = 41288647.12 Elapsed time : 730 sec\n",
      "Training // Epoch 84 //  Total cost = 40523911.88 Elapsed time : 739 sec\n",
      "Training // Epoch 85 //  Total cost = 39775534.69 Elapsed time : 747 sec\n",
      "Training // Epoch 86 //  Total cost = 39051408.12 Elapsed time : 756 sec\n",
      "Training // Epoch 87 //  Total cost = 38350553.22 Elapsed time : 764 sec\n",
      "Training // Epoch 88 //  Total cost = 37661407.00 Elapsed time : 775 sec\n",
      "Training // Epoch 89 //  Total cost = 36996213.59 Elapsed time : 783 sec\n",
      "Training // Epoch 90 //  Total cost = 36349649.47 Elapsed time : 791 sec\n",
      "Training // Epoch 91 //  Total cost = 35726645.00 Elapsed time : 800 sec\n",
      "Training // Epoch 92 //  Total cost = 35117365.41 Elapsed time : 811 sec\n",
      "Training // Epoch 93 //  Total cost = 34530359.38 Elapsed time : 822 sec\n",
      "Training // Epoch 94 //  Total cost = 33960657.64 Elapsed time : 832 sec\n",
      "Training // Epoch 95 //  Total cost = 33409376.77 Elapsed time : 841 sec\n",
      "Training // Epoch 96 //  Total cost = 32873193.94 Elapsed time : 850 sec\n",
      "Training // Epoch 97 //  Total cost = 32355168.34 Elapsed time : 858 sec\n",
      "Training // Epoch 98 //  Total cost = 31859824.70 Elapsed time : 867 sec\n",
      "Training // Epoch 99 //  Total cost = 31375273.75 Elapsed time : 876 sec\n",
      "Training // Epoch 100 //  Total cost = 30909934.75 Elapsed time : 885 sec\n",
      "Training // Epoch 101 //  Total cost = 30461370.30 Elapsed time : 893 sec\n",
      "Training // Epoch 102 //  Total cost = 30028708.03 Elapsed time : 902 sec\n",
      "Training // Epoch 103 //  Total cost = 29610802.08 Elapsed time : 911 sec\n",
      "Training // Epoch 104 //  Total cost = 29209048.72 Elapsed time : 920 sec\n",
      "Training // Epoch 105 //  Total cost = 28818532.34 Elapsed time : 928 sec\n",
      "Training // Epoch 106 //  Total cost = 28447847.41 Elapsed time : 937 sec\n",
      "Training // Epoch 107 //  Total cost = 28091455.08 Elapsed time : 946 sec\n",
      "Training // Epoch 108 //  Total cost = 27748029.56 Elapsed time : 955 sec\n",
      "Training // Epoch 109 //  Total cost = 27415978.22 Elapsed time : 963 sec\n",
      "Training // Epoch 110 //  Total cost = 27100371.06 Elapsed time : 972 sec\n",
      "Training // Epoch 111 //  Total cost = 26795215.59 Elapsed time : 981 sec\n",
      "Training // Epoch 112 //  Total cost = 26506069.69 Elapsed time : 990 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training // Epoch 113 //  Total cost = 26229822.77 Elapsed time : 999 sec\n",
      "Training // Epoch 114 //  Total cost = 25965280.67 Elapsed time : 1008 sec\n",
      "Training // Epoch 115 //  Total cost = 25713353.86 Elapsed time : 1017 sec\n",
      "Training // Epoch 116 //  Total cost = 25468158.66 Elapsed time : 1025 sec\n",
      "Training // Epoch 117 //  Total cost = 25241022.56 Elapsed time : 1033 sec\n",
      "Training // Epoch 118 //  Total cost = 25019519.48 Elapsed time : 1042 sec\n",
      "Training // Epoch 119 //  Total cost = 24811663.63 Elapsed time : 1050 sec\n",
      "Training // Epoch 120 //  Total cost = 24609974.30 Elapsed time : 1058 sec\n",
      "Training // Epoch 121 //  Total cost = 24420480.23 Elapsed time : 1066 sec\n",
      "Training // Epoch 122 //  Total cost = 24235782.49 Elapsed time : 1074 sec\n",
      "Training // Epoch 123 //  Total cost = 24066398.74 Elapsed time : 1082 sec\n",
      "Training // Epoch 124 //  Total cost = 23899705.29 Elapsed time : 1091 sec\n",
      "Training // Epoch 125 //  Total cost = 23745025.42 Elapsed time : 1099 sec\n",
      "Training // Epoch 126 //  Total cost = 23600273.58 Elapsed time : 1107 sec\n",
      "Training // Epoch 127 //  Total cost = 23457817.05 Elapsed time : 1115 sec\n",
      "Training // Epoch 128 //  Total cost = 23326691.69 Elapsed time : 1123 sec\n",
      "Training // Epoch 129 //  Total cost = 23200199.76 Elapsed time : 1132 sec\n",
      "Training // Epoch 130 //  Total cost = 23082377.88 Elapsed time : 1140 sec\n",
      "Training // Epoch 131 //  Total cost = 22968153.27 Elapsed time : 1148 sec\n",
      "Training // Epoch 132 //  Total cost = 22860444.31 Elapsed time : 1156 sec\n",
      "Training // Epoch 133 //  Total cost = 22760058.06 Elapsed time : 1164 sec\n",
      "Training // Epoch 134 //  Total cost = 22666147.52 Elapsed time : 1172 sec\n",
      "Training // Epoch 135 //  Total cost = 22571107.13 Elapsed time : 1180 sec\n",
      "Training // Epoch 136 //  Total cost = 22487584.22 Elapsed time : 1188 sec\n",
      "Training // Epoch 137 //  Total cost = 22405833.73 Elapsed time : 1196 sec\n",
      "Training // Epoch 138 //  Total cost = 22330392.38 Elapsed time : 1205 sec\n",
      "Training // Epoch 139 //  Total cost = 22255380.34 Elapsed time : 1213 sec\n",
      "Training // Epoch 140 //  Total cost = 22186002.29 Elapsed time : 1221 sec\n",
      "Training // Epoch 141 //  Total cost = 22117630.56 Elapsed time : 1229 sec\n",
      "Training // Epoch 142 //  Total cost = 22055342.34 Elapsed time : 1237 sec\n",
      "Training // Epoch 143 //  Total cost = 21993179.08 Elapsed time : 1245 sec\n",
      "Training // Epoch 144 //  Total cost = 21932841.30 Elapsed time : 1253 sec\n",
      "Training // Epoch 145 //  Total cost = 21884063.32 Elapsed time : 1262 sec\n",
      "Training // Epoch 146 //  Total cost = 21835091.59 Elapsed time : 1270 sec\n",
      "Training // Epoch 147 //  Total cost = 21789127.23 Elapsed time : 1278 sec\n",
      "Training // Epoch 148 //  Total cost = 21742637.62 Elapsed time : 1286 sec\n",
      "Training // Epoch 149 //  Total cost = 21705565.35 Elapsed time : 1294 sec\n",
      "Training // Epoch 150 //  Total cost = 21668287.75 Elapsed time : 1303 sec\n",
      "Training // Epoch 151 //  Total cost = 21630913.35 Elapsed time : 1312 sec\n",
      "Training // Epoch 152 //  Total cost = 21599348.62 Elapsed time : 1320 sec\n",
      "Training // Epoch 153 //  Total cost = 21571637.19 Elapsed time : 1328 sec\n",
      "Training // Epoch 154 //  Total cost = 21543501.29 Elapsed time : 1336 sec\n",
      "Training // Epoch 155 //  Total cost = 21513418.12 Elapsed time : 1345 sec\n",
      "Training // Epoch 156 //  Total cost = 21485080.34 Elapsed time : 1353 sec\n",
      "Training // Epoch 157 //  Total cost = 21438892.87 Elapsed time : 1363 sec\n",
      "Training // Epoch 158 //  Total cost = 21377036.28 Elapsed time : 1372 sec\n",
      "Training // Epoch 159 //  Total cost = 21229306.30 Elapsed time : 1381 sec\n",
      "Training // Epoch 160 //  Total cost = 21071121.86 Elapsed time : 1389 sec\n",
      "Training // Epoch 161 //  Total cost = 20925566.02 Elapsed time : 1398 sec\n",
      "Training // Epoch 162 //  Total cost = 20836658.01 Elapsed time : 1406 sec\n",
      "Training // Epoch 163 //  Total cost = 20774302.80 Elapsed time : 1415 sec\n",
      "Training // Epoch 164 //  Total cost = 20716914.44 Elapsed time : 1424 sec\n",
      "Training // Epoch 165 //  Total cost = 20707060.63 Elapsed time : 1432 sec\n",
      "Training // Epoch 166 //  Total cost = 20661099.41 Elapsed time : 1441 sec\n",
      "Training // Epoch 167 //  Total cost = 20625338.57 Elapsed time : 1451 sec\n",
      "Training // Epoch 168 //  Total cost = 20589319.52 Elapsed time : 1460 sec\n",
      "Training // Epoch 169 //  Total cost = 20572881.80 Elapsed time : 1469 sec\n",
      "Training // Epoch 170 //  Total cost = 20567003.31 Elapsed time : 1477 sec\n",
      "Training // Epoch 171 //  Total cost = 20557453.87 Elapsed time : 1485 sec\n",
      "Training // Epoch 172 //  Total cost = 20514101.95 Elapsed time : 1494 sec\n",
      "Training // Epoch 173 //  Total cost = 20469838.30 Elapsed time : 1502 sec\n",
      "Training // Epoch 174 //  Total cost = 20426545.98 Elapsed time : 1511 sec\n",
      "Training // Epoch 175 //  Total cost = 20403331.02 Elapsed time : 1519 sec\n",
      "Training // Epoch 176 //  Total cost = 20382709.72 Elapsed time : 1527 sec\n",
      "Training // Epoch 177 //  Total cost = 20339543.34 Elapsed time : 1535 sec\n",
      "Training // Epoch 178 //  Total cost = 20291659.41 Elapsed time : 1543 sec\n",
      "Training // Epoch 179 //  Total cost = 20293770.04 Elapsed time : 1551 sec\n",
      "Training // Epoch 180 //  Total cost = 20236462.23 Elapsed time : 1560 sec\n",
      "Training // Epoch 181 //  Total cost = 20202695.11 Elapsed time : 1568 sec\n",
      "Training // Epoch 182 //  Total cost = 20165418.95 Elapsed time : 1576 sec\n",
      "Training // Epoch 183 //  Total cost = 20137667.19 Elapsed time : 1584 sec\n",
      "Training // Epoch 184 //  Total cost = 20112504.45 Elapsed time : 1592 sec\n",
      "Training // Epoch 185 //  Total cost = 20064062.89 Elapsed time : 1600 sec\n",
      "Training // Epoch 186 //  Total cost = 20035768.56 Elapsed time : 1609 sec\n",
      "Training // Epoch 187 //  Total cost = 20022080.76 Elapsed time : 1617 sec\n",
      "Training // Epoch 188 //  Total cost = 19988423.27 Elapsed time : 1625 sec\n",
      "Training // Epoch 189 //  Total cost = 19961282.66 Elapsed time : 1633 sec\n",
      "Training // Epoch 190 //  Total cost = 19926572.26 Elapsed time : 1641 sec\n",
      "Training // Epoch 191 //  Total cost = 19882042.70 Elapsed time : 1649 sec\n",
      "Training // Epoch 192 //  Total cost = 19857866.74 Elapsed time : 1658 sec\n",
      "Training // Epoch 193 //  Total cost = 19820119.57 Elapsed time : 1666 sec\n",
      "Training // Epoch 194 //  Total cost = 19791277.95 Elapsed time : 1674 sec\n",
      "Training // Epoch 195 //  Total cost = 19769504.04 Elapsed time : 1682 sec\n",
      "Training // Epoch 196 //  Total cost = 19738930.83 Elapsed time : 1690 sec\n",
      "Training // Epoch 197 //  Total cost = 19727672.70 Elapsed time : 1698 sec\n",
      "Training // Epoch 198 //  Total cost = 19704796.80 Elapsed time : 1706 sec\n",
      "Training // Epoch 199 //  Total cost = 19676641.04 Elapsed time : 1715 sec\n",
      "Training // Epoch 200 //  Total cost = 19652868.22 Elapsed time : 1723 sec\n"
     ]
    }
   ],
   "source": [
    "R_train = rating_matrix.copy()\n",
    "cdl = CDL(R_train , item_infomation_matrix)\n",
    "cdl.build_model()\n",
    "R = cdl.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnt = 0\n",
    "for i in range(rating_matrix.shape[0]):\n",
    "    l_score = np.ravel(R[i,:]).tolist()\n",
    "    pl = sorted(enumerate(l_score),key=lambda d:d[1],reverse=True)\n",
    "    l_rec = [i[0] for i in pl][:300]\n",
    "    s_rec = set(l_rec)\n",
    "    s_true = set(np.ravel(np.where(rating_matrix[i,:]>0)))\n",
    "    cnt_hit = len(s_rec.intersection(s_true))\n",
    "    all_cnt = all_cnt + cnt_hit/len(s_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.081\n"
     ]
    }
   ],
   "source": [
    "#accuracy 0.085不能算太低 因為他是所有item(16980)去排序\n",
    "print(\"accuracy : %.3f\"%(all_cnt/rating_matrix.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
