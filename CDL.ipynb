{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k12s35h813g/.conda/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "#init random seed\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build item information matrix of citeulike-a by bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find vocabulary_size = 8000\n",
    "with open(r\"ctrsr_datasets/citeulike-a/vocabulary.dat\") as vocabulary_file:\n",
    "    vocabulary_size = len(vocabulary_file.readlines())\n",
    "    \n",
    "#find item_size = 16980\n",
    "with open(r\"ctrsr_datasets/citeulike-a/mult.dat\") as item_info_file:\n",
    "    item_size = len(item_info_file.readlines())\n",
    "\n",
    "#initialize item_infomation_matrix (16980 , 8000)\n",
    "item_infomation_matrix = np.zeros((item_size , vocabulary_size))\n",
    "\n",
    "#build item_infomation_matrix\n",
    "with open(r\"ctrsr_datasets/citeulike-a/mult.dat\") as item_info_file:\n",
    "    sentences = item_info_file.readlines()\n",
    "    \n",
    "    for index,sentence in enumerate(sentences):\n",
    "        words = sentence.strip().split(\" \")[1:]\n",
    "        for word in words:\n",
    "            vocabulary_index , number = word.split(\":\")\n",
    "            item_infomation_matrix[index][int(vocabulary_index)] =number\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build rating matrix citeulike-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find user_size = 5551\n",
    "with open(r\"ctrsr_datasets/citeulike-a/users.dat\") as rating_file:\n",
    "    user_size = len(rating_file.readlines())\n",
    "\n",
    "#initialize rating_matrix (5551 , 16980)\n",
    "import numpy as np\n",
    "rating_matrix = np.zeros((user_size , item_size))\n",
    "\n",
    "#build rating_matrix\n",
    "with open(r\"ctrsr_datasets/citeulike-a/users.dat\") as rating_file:\n",
    "    lines = rating_file.readlines()\n",
    "    for index,line in enumerate(lines):\n",
    "        items = line.strip().split(\" \")\n",
    "        for item in items:  \n",
    "            rating_matrix[index][int(item)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save matrix by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'item_infomation_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(item_infomation_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(r'rating_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(rating_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load matrix from pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'item_infomation_matrix.pickle', 'rb') as handle:\n",
    "    item_infomation_matrix = pickle.load(handle)  \n",
    "    \n",
    "with open(r'rating_matrix.pickle', 'rb') as handle2:\n",
    "    rating_matrix = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(corruption_level ,size):\n",
    "    mask = np.random.binomial(1, 1 - corruption_level, [size[0],size[1]])\n",
    "    return mask\n",
    "\n",
    "def add_noise(x , corruption_level ):\n",
    "    x = x * mask(corruption_level , x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDL():\n",
    "    def __init__(self , rating_matrix , item_infomation_matrix):\n",
    "        # model參數設定\n",
    "        self.n_input = 8000\n",
    "        self.n_hidden1 = 200\n",
    "        self.n_hidden2 = 50\n",
    "        self.k = 50\n",
    "        \n",
    "        self.lambda_w = 1\n",
    "        self.lambda_n = 1\n",
    "        self.lambda_u = 1\n",
    "        self.lambda_v = 1\n",
    "        \n",
    "        self.drop_ratio = 0.1\n",
    "        self.learning_rate = 0.001\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.num_u = rating_matrix.shape[0]\n",
    "        self.num_v = rating_matrix.shape[1]\n",
    "        \n",
    "        self.Weights = {\n",
    "            'w1' : tf.Variable(tf.random_normal( [self.n_input , self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'w2' : tf.Variable(tf.random_normal( [self.n_hidden1 , self.n_hidden2] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'w3' : tf.Variable(tf.random_normal( [self.n_hidden2 , self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'w4' : tf.Variable(tf.random_normal( [self.n_hidden1 , self.n_input] , mean=0.0, stddev=1 / self.lambda_w ))   \n",
    "        }\n",
    "        self.Biases = {\n",
    "            'b1' : tf.Variable(tf.random_normal( [self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b2' : tf.Variable(tf.random_normal( [self.n_hidden2] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b3' : tf.Variable(tf.random_normal( [self.n_hidden1] , mean=0.0, stddev=1 / self.lambda_w )),\n",
    "            'b4' : tf.Variable(tf.random_normal( [self.n_input] , mean=0.0, stddev=1 / self.lambda_w ))\n",
    "        }\n",
    "        \n",
    "        self.item_infomation_matrix = item_infomation_matrix\n",
    "    \n",
    "        self.build_model()\n",
    "    def encoder(self , x , drop_ratio):\n",
    "        w1 = self.Weights['w1']\n",
    "        b1 = self.Biases['b1']\n",
    "        L1 = tf.nn.sigmoid( tf.matmul(x,w1) + b1 )\n",
    "        L1 = tf.nn.dropout( L1 , keep_prob= 1 - drop_ratio )\n",
    "        \n",
    "        w2 = self.Weights['w2']\n",
    "        b2 = self.Biases['b2']\n",
    "        L2 = tf.nn.sigmoid( tf.matmul(L1,w2) + b2 )\n",
    "        L2 = tf.nn.dropout(L2 , keep_prob= 1 - drop_ratio)\n",
    "        \n",
    "        return L2\n",
    "    \n",
    "    def decoder(self , x , drop_ratio):\n",
    "        w3 = self.Weights['w3']\n",
    "        b3 = self.Biases['b3']\n",
    "        L3 = tf.nn.sigmoid(tf.matmul(x,w3) + b3)\n",
    "        L3 = tf.nn.dropout(L3 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        w4 = self.Weights['w4']\n",
    "        b4 = self.Biases['b4']\n",
    "        L4 = tf.nn.sigmoid(tf.matmul(L3,w4) + b4)\n",
    "        L4 = tf.nn.dropout(L4 , keep_prob= 1 - drop_ratio)\n",
    "\n",
    "        return L4\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model_X_0 = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.model_X_c = tf.placeholder(tf.float32 , shape=(None , self.n_input))\n",
    "        self.model_V = tf.placeholder(tf.float32 , shape=(None , self.k))\n",
    "        self.model_drop_ratio = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.V_sdae = self.encoder( self.model_X_0 , self.model_drop_ratio )\n",
    "        self.y_pred = self.decoder( self.V_sdae , self.model_drop_ratio )\n",
    "        \n",
    "        self.Regularization = tf.reduce_sum([tf.nn.l2_loss(w)+tf.nn.l2_loss(b) for w,b in zip(self.Weights.values() , self.Biases.values())])\n",
    "        loss_r =1/2 * self.lambda_w * self.Regularization\n",
    "        loss_a =1/2 * self.lambda_n * tf.reduce_sum(tf.pow( self.model_X_c - self.y_pred , 2 ))\n",
    "        loss_v =1/2 * self.lambda_v * tf.reduce_sum(tf.pow( self.model_V - self.V_sdae , 2 ))\n",
    "        self.Loss = loss_r + loss_a + loss_v\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.Loss)\n",
    "    def training(self , rating_matrix):\n",
    "        #np.random.shuffle(self.item_infomation_matrix) #random index of train data\n",
    "        \n",
    "        self.item_infomation_matrix_noise = add_noise(self.item_infomation_matrix , 0.3)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        mf = MF( rating_matrix )\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"%d / %d\"%(epoch+1 , self.epochs))\n",
    "            \n",
    "            V_sdae = sess.run(self.V_sdae , feed_dict={self.model_X_0 : self.item_infomation_matrix_noise , self.model_drop_ratio : 0.1})\n",
    "            \n",
    "            U , V = mf.ALS(V_sdae)\n",
    "            V = np.resize(V,(16980 , 50))\n",
    "            for i in range(0 , self.item_infomation_matrix.shape[0] , self.batch_size):\n",
    "                X_train_batch = self.item_infomation_matrix_noise[i:i+self.batch_size]\n",
    "                y_train_batch = self.item_infomation_matrix[i:i+self.batch_size]\n",
    "                V_batch = V[i:i+self.batch_size]\n",
    "                _ , my_loss = sess.run([self.optimizer, self.Loss] , feed_dict={self.model_X_0 :X_train_batch , self.model_X_c : y_train_batch , self.model_V:V_batch, self.model_drop_ratio : 0.1})\n",
    "            print(my_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 100\n",
      "387160.1\n",
      "2 / 100\n",
      "174808.58\n",
      "3 / 100\n",
      "76476.7\n",
      "4 / 100\n",
      "33072.277\n",
      "5 / 100\n",
      "14402.748\n",
      "6 / 100\n",
      "6735.0117\n",
      "7 / 100\n",
      "3759.189\n",
      "8 / 100\n",
      "2656.2021\n",
      "9 / 100\n",
      "2307.793\n",
      "10 / 100\n",
      "2172.792\n",
      "11 / 100\n",
      "2184.463\n",
      "12 / 100\n",
      "2132.5603\n",
      "13 / 100\n",
      "2181.7986\n",
      "14 / 100\n",
      "2125.1125\n",
      "15 / 100\n",
      "2208.9133\n",
      "16 / 100\n",
      "2123.6982\n",
      "17 / 100\n",
      "2194.285\n",
      "18 / 100\n",
      "2111.4626\n",
      "19 / 100\n",
      "2199.0996\n",
      "20 / 100\n",
      "2124.3508\n",
      "21 / 100\n",
      "2210.195\n",
      "22 / 100\n",
      "2118.1206\n",
      "23 / 100\n",
      "2205.9675\n",
      "24 / 100\n",
      "2117.697\n",
      "25 / 100\n",
      "2194.4336\n",
      "26 / 100\n",
      "2118.0552\n",
      "27 / 100\n",
      "2205.803\n",
      "28 / 100\n",
      "2111.7512\n",
      "29 / 100\n",
      "2208.225\n",
      "30 / 100\n",
      "2123.2844\n",
      "31 / 100\n",
      "2211.9521\n",
      "32 / 100\n",
      "2110.9543\n",
      "33 / 100\n",
      "2213.4731\n",
      "34 / 100\n",
      "2114.7104\n",
      "35 / 100\n",
      "2209.4028\n",
      "36 / 100\n",
      "2114.4802\n",
      "37 / 100\n",
      "2214.9265\n",
      "38 / 100\n",
      "2114.7622\n",
      "39 / 100\n",
      "2213.9478\n",
      "40 / 100\n",
      "2118.1965\n",
      "41 / 100\n",
      "2216.033\n",
      "42 / 100\n",
      "2121.04\n",
      "43 / 100\n",
      "2225.5112\n",
      "44 / 100\n",
      "2124.3408\n",
      "45 / 100\n",
      "2222.1272\n",
      "46 / 100\n",
      "2113.198\n",
      "47 / 100\n",
      "2229.3225\n",
      "48 / 100\n",
      "2113.7095\n",
      "49 / 100\n",
      "2224.6775\n",
      "50 / 100\n",
      "2121.2583\n",
      "51 / 100\n",
      "2218.532\n",
      "52 / 100\n",
      "2108.2317\n",
      "53 / 100\n",
      "2215.686\n",
      "54 / 100\n",
      "2119.0842\n",
      "55 / 100\n",
      "2209.243\n",
      "56 / 100\n",
      "2110.7441\n",
      "57 / 100\n",
      "2202.0361\n",
      "58 / 100\n",
      "2119.9165\n",
      "59 / 100\n",
      "2209.9695\n",
      "60 / 100\n",
      "2117.3057\n",
      "61 / 100\n",
      "2195.625\n",
      "62 / 100\n",
      "2106.9358\n",
      "63 / 100\n",
      "2188.9092\n",
      "64 / 100\n",
      "2109.2012\n",
      "65 / 100\n",
      "2181.3408\n",
      "66 / 100\n",
      "2123.1904\n",
      "67 / 100\n",
      "2182.553\n",
      "68 / 100\n",
      "2115.4556\n",
      "69 / 100\n",
      "2182.2495\n",
      "70 / 100\n",
      "2126.6855\n",
      "71 / 100\n",
      "2171.3242\n",
      "72 / 100\n",
      "2115.0232\n",
      "73 / 100\n",
      "2168.794\n",
      "74 / 100\n",
      "2127.583\n",
      "75 / 100\n",
      "2162.3296\n",
      "76 / 100\n",
      "2121.961\n",
      "77 / 100\n",
      "2164.2263\n",
      "78 / 100\n",
      "2126.7412\n",
      "79 / 100\n",
      "2156.61\n",
      "80 / 100\n",
      "2114.6692\n",
      "81 / 100\n",
      "2156.3647\n",
      "82 / 100\n",
      "2113.5364\n",
      "83 / 100\n",
      "2166.9353\n",
      "84 / 100\n",
      "2123.1746\n",
      "85 / 100\n",
      "2146.9045\n",
      "86 / 100\n",
      "2125.6362\n",
      "87 / 100\n",
      "2152.5269\n",
      "88 / 100\n",
      "2116.7075\n",
      "89 / 100\n",
      "2159.4104\n",
      "90 / 100\n",
      "2121.5642\n",
      "91 / 100\n",
      "2153.7263\n",
      "92 / 100\n",
      "2117.1426\n",
      "93 / 100\n",
      "2150.8237\n",
      "94 / 100\n",
      "2115.8794\n",
      "95 / 100\n",
      "2154.5176\n",
      "96 / 100\n",
      "2116.168\n",
      "97 / 100\n",
      "2142.4043\n",
      "98 / 100\n",
      "2118.486\n",
      "99 / 100\n",
      "2151.7048\n",
      "100 / 100\n",
      "2121.3064\n"
     ]
    }
   ],
   "source": [
    "cdl = CDL(rating_matrix , item_infomation_matrix)\n",
    "cdl.build_model()\n",
    "cdl.training(rating_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matrix factorization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF():\n",
    "    def __init__(self , rating_matrix ):\n",
    "        #### 參數設定\n",
    "        self.num_u = rating_matrix.shape[0] #5551\n",
    "        self.num_v = rating_matrix.shape[1] #16980\n",
    "        self.u_lambda = 100\n",
    "        self.v_lambda = 0.1\n",
    "        self.k = 50 #latent維度\n",
    "        self.a = 1\n",
    "        self.b =0.01\n",
    "        self.R = np.mat(rating_matrix)\n",
    "        self.C = np.mat(np.ones(self.R.shape)) * self.b\n",
    "        self.C[np.where(self.R>0)] = self.a\n",
    "        self.I_U = np.mat(np.eye(self.k) * self.u_lambda)\n",
    "        self.I_V = np.mat(np.eye(self.k) * self.v_lambda)\n",
    "        self.U = np.mat(np.random.normal(0 , 1/self.u_lambda , size=(self.k,self.num_u)))\n",
    "        self.V = np.mat(np.random.normal(0 , 1/self.v_lambda , size=(self.k,self.num_v)))\n",
    "                        \n",
    "\n",
    "    def test(self):\n",
    "        print( ((U_cut*self.R[np.ravel(np.where(self.R[:,j]>0)[1]),j] + self.v_lambda * self.V_sdae[j])).shape)\n",
    "    def ALS(self , V_sdae):\n",
    "        self.V_sdae = np.mat(V_sdae)\n",
    "        \n",
    "        V_sq = self.V * self.V.T * self.b\n",
    "        for i in range(self.num_u):\n",
    "            idx_a = np.ravel(np.where(self.R[i,:]>0)[1])\n",
    "            V_cut = self.V[:,idx_a]\n",
    "            self.U[:,i] = np.linalg.pinv( V_sq+ V_cut * V_cut.T * (self.a-self.b) + self.I_U )*(V_cut*self.R[i,idx_a].T) #V_sq+V_cut*V_cut.T*a_m_b = VCV^T\n",
    "        \n",
    "        U_sq = self.U * self.U.T * self.b\n",
    "        for j in range(self.num_v):\n",
    "            idx_a = np.ravel(np.where(self.R[:,j]>0)[1])\n",
    "            U_cut = self.U[:,idx_a]\n",
    "            self.V[:,j] = np.linalg.pinv(U_sq+U_cut*U_cut.T*(self.a-self.b)+self.I_V)* (U_cut*self.R[idx_a,j] + self.v_lambda * np.resize(self.V_sdae[j],(self.k,1)))\n",
    "        \n",
    "        return self.U ,self.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MF( rating_matrix , V )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "U , V = mf.ALS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_U = tf.placeholder(tf.float32 , shape=(n_user,k))\n",
    "tf_V = tf.placeholder(tf.float32 , shape=(k,n_item))\n",
    "\n",
    "c = tf.placeholder(tf.float32 , shape=(n_item))\n",
    "r = tf.placeholder(tf.float32 , shape=(n_item))\n",
    "\n",
    "c1 = tf.placeholder(tf.float32 , shape=(n_user))\n",
    "r1 = tf.placeholder(tf.float32 , shape=(n_user))\n",
    "\n",
    "mul1 = tf.matrix_inverse( tf.cast(tf.matmul ( tf.matmul(tf_V, tf.diag(c)) , tf.transpose(tf_V) ), tf.float32) + u_lambda * tf.eye(k) )\n",
    "\n",
    "mul2 = tf.cast(tf.reduce_sum(tf.multiply(tf.matmul (tf_V , tf.diag(c) ) , r) , reduction_indices=1),dtype=tf.float32)\n",
    "\n",
    "mul3 = tf.reduce_sum(tf.multiply(mul1 , mul2) , reduction_indices=1)\n",
    "\n",
    "mul4 = tf.matrix_inverse( tf.cast(tf.matmul ( tf.matmul(tf.transpose(tf_U), tf.diag(c1)) , tf_U ), tf.float32) + v_lambda * tf.eye(k) )\n",
    "\n",
    "mul5 = tf.cast(tf.reduce_sum(tf.multiply(tf.matmul (tf.transpose(tf_U) , tf.diag(c1) ) , r1) , reduction_indices=1),dtype=tf.float32)\n",
    "\n",
    "mul6 = tf.reduce_sum(tf.multiply(mul4 , mul5) , reduction_indices=1)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for e in range(20):\n",
    "    print(e)\n",
    "\n",
    "    \n",
    "    for i in range(n_user):\n",
    "        U[i] = sess.run(mul3 , feed_dict={c:confidence_matrix[i] , r:rating_matrix[i] , tf_U:U , tf_V:V})\n",
    "\n",
    "    \n",
    "    for j in range(n_item):\n",
    "        V[:,j] = sess.run(mul6 , feed_dict={c1:confidence_matrix.T[j] , r1:rating_matrix.T[j] , tf_U:U , tf_V:V})\n",
    "        \n",
    "    r_ = np.dot(U , V)\n",
    "    loss = np.square(np.subtract(r_, rating_matrix)).sum()\n",
    "    print(loss)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
