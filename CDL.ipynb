{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build item information matrix of citeulike-a by bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find vocabulary_size = 8000\n",
    "with open(r\"ctrsr_datasets/citeulike-a/vocabulary.dat\") as vocabulary_file:\n",
    "    vocabulary_size = len(vocabulary_file.readlines())\n",
    "    \n",
    "#find item_size = 16980\n",
    "with open(r\"ctrsr_datasets/citeulike-a/mult.dat\") as item_info_file:\n",
    "    item_size = len(item_info_file.readlines())\n",
    "\n",
    "#initialize item_infomation_matrix (16980 , 8000)\n",
    "import numpy as np\n",
    "item_infomation_matrix = np.zeros((item_size , vocabulary_size))\n",
    "\n",
    "#build item_infomation_matrix\n",
    "with open(r\"ctrsr_datasets/citeulike-a/mult.dat\") as item_info_file:\n",
    "    sentences = item_info_file.readlines()\n",
    "    \n",
    "    for index,sentence in enumerate(sentences):\n",
    "        words = sentence.strip().split(\" \")[1:]\n",
    "        for word in words:\n",
    "            vocabulary_index , number = word.split(\":\")\n",
    "            item_infomation_matrix[index][int(vocabulary_index)] =number\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build rating matrix citeulike-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find user_size = 5551\n",
    "with open(r\"ctrsr_datasets/citeulike-a/users.dat\") as rating_file:\n",
    "    user_size = len(rating_file.readlines())\n",
    "\n",
    "#initialize rating_matrix (5551 , 16980)\n",
    "import numpy as np\n",
    "rating_matrix = np.zeros((user_size , item_size))\n",
    "\n",
    "#build rating_matrix\n",
    "with open(r\"ctrsr_datasets/citeulike-a/users.dat\") as rating_file:\n",
    "    lines = rating_file.readlines()\n",
    "    for index,line in enumerate(lines):\n",
    "        items = line.strip().split(\" \")\n",
    "        for item in items:  \n",
    "            rating_matrix[index][int(item)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save matrix by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'item_infomation_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(item_infomation_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(r'rating_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(rating_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init random seed\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load matrix from pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'item_infomation_matrix.pickle', 'rb') as handle:\n",
    "    item_infomation_matrix = pickle.load(handle)  \n",
    "    \n",
    "with open(r'rating_matrix.pickle', 'rb') as handle2:\n",
    "    rating_matrix = pickle.load(handle2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 8000\n",
    "n_hidden1 = 200\n",
    "n_hidden2 = 50\n",
    "\n",
    "lambda_w = 1\n",
    "lambda_j = 1\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = tf.placeholder(tf.float32 , shape=(None , n_input))\n",
    "drop_ratio = tf.placeholder(tf.float32 )\n",
    "X_c = tf.placeholder(tf.float32 , shape=(None , n_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x , drop_ratio):\n",
    "    w_1 = tf.Variable(tf.random_normal( [n_input , n_hidden1] , mean=0.0, stddev=1 / lambda_w ))\n",
    "    b_1 = tf.Variable(tf.random_normal( [n_hidden1] , mean=0.0, stddev=1 / lambda_w ))\n",
    "    L_1 = tf.nn.sigmoid(tf.matmul(x,w_1) + b_1)\n",
    "    L_1 = tf.nn.dropout(L_1 , keep_prob= 1 - drop_ratio)\n",
    "    \n",
    "    w_2 = tf.Variable(tf.random_normal( [n_hidden1 , n_hidden2] , mean=0.0, stddev=1 / lambda_w ))\n",
    "    b_2 = tf.Variable(tf.random_normal( [n_hidden2] , mean=0.0, stddev=1 / lambda_w ))\n",
    "    L_2 = tf.nn.sigmoid(tf.matmul(L_1,w_2) + b_2)\n",
    "    L_2 = tf.nn.dropout(L_2 , keep_prob= 1 - drop_ratio)\n",
    "    \n",
    "    return L_2\n",
    "\n",
    "def decoder(x , drop_ratio):\n",
    "    w_1 = tf.Variable(tf.random_normal( [n_hidden2 , n_hidden1] , mean=0.0, stddev=1 / lambda_w ))\n",
    "    b_1 = tf.Variable(tf.random_normal( [n_hidden1] , mean=0.0, stddev=1 / lambda_w ))\n",
    "    L_1 = tf.nn.sigmoid(tf.matmul(x,w_1) + b_1)\n",
    "    L_1 = tf.nn.dropout(L_1 , keep_prob= 1 - drop_ratio)\n",
    "    \n",
    "    w_2 = tf.Variable(tf.random_normal( [n_hidden1 , n_input] , mean=0.0, stddev=1 / lambda_w ))\n",
    "    b_2 = tf.Variable(tf.random_normal( [n_input] , mean=0.0, stddev=1 / lambda_w ))\n",
    "    L_2 = tf.nn.sigmoid(tf.matmul(L_1,w_2) + b_2)\n",
    "    L_2 = tf.nn.dropout(L_2 , keep_prob= 1 - drop_ratio)\n",
    "    \n",
    "    return L_2\n",
    "\n",
    "def stacked_autoencoder(x , drop_ratio):\n",
    "    encoder_output = encoder(x , drop_ratio)\n",
    "    decoder_output = decoder(encoder_output , drop_ratio)\n",
    "    #decoder_output = tf.Variable(tf.random_normal( shape=(n_input) , mean=decoder_output, stddev=1 / lambda_j ))\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stacked_autoencoder(X_0 , drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean( tf.pow( X_c - y_pred, 2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### masking noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(corruption_level ,size):\n",
    "    mask = np.random.binomial(1, 1 - corruption_level, [size[0],size[1]])\n",
    "    return mask\n",
    "\n",
    "def add_noise(x , corruption_level ):\n",
    "    x = x * mask(corruption_level , x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trainging SDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 100\n",
      "0.0408685\n",
      "2 / 100\n",
      "0.03735322\n",
      "3 / 100\n",
      "0.036900964\n",
      "4 / 100\n",
      "0.03623298\n",
      "5 / 100\n",
      "0.036303572\n",
      "6 / 100\n",
      "0.035621922\n",
      "7 / 100\n",
      "0.035176165\n",
      "8 / 100\n",
      "0.034799315\n",
      "9 / 100\n",
      "0.03430466\n",
      "10 / 100\n",
      "0.034051135\n",
      "11 / 100\n",
      "0.033710763\n",
      "12 / 100\n",
      "0.033708513\n",
      "13 / 100\n",
      "0.033184234\n",
      "14 / 100\n",
      "0.032846563\n",
      "15 / 100\n",
      "0.032619923\n",
      "16 / 100\n",
      "0.03191311\n",
      "17 / 100\n",
      "0.031550966\n",
      "18 / 100\n",
      "0.030927423\n",
      "19 / 100\n",
      "0.030331863\n",
      "20 / 100\n",
      "0.029995788\n",
      "21 / 100\n",
      "0.029391833\n",
      "22 / 100\n",
      "0.028713318\n",
      "23 / 100\n",
      "0.028577851\n",
      "24 / 100\n",
      "0.028025744\n",
      "25 / 100\n",
      "0.027895307\n",
      "26 / 100\n",
      "0.027624547\n",
      "27 / 100\n",
      "0.027608208\n",
      "28 / 100\n",
      "0.027472727\n",
      "29 / 100\n",
      "0.0272853\n",
      "30 / 100\n",
      "0.02730599\n",
      "31 / 100\n",
      "0.027276788\n",
      "32 / 100\n",
      "0.02719413\n",
      "33 / 100\n",
      "0.027098432\n",
      "34 / 100\n",
      "0.026884759\n",
      "35 / 100\n",
      "0.026618829\n",
      "36 / 100\n",
      "0.02652392\n",
      "37 / 100\n",
      "0.026382416\n",
      "38 / 100\n",
      "0.02623517\n",
      "39 / 100\n",
      "0.02613476\n",
      "40 / 100\n",
      "0.026049793\n",
      "41 / 100\n",
      "0.02596328\n",
      "42 / 100\n",
      "0.025791943\n",
      "43 / 100\n",
      "0.025933292\n",
      "44 / 100\n",
      "0.0257954\n",
      "45 / 100\n",
      "0.025714237\n",
      "46 / 100\n",
      "0.025537478\n",
      "47 / 100\n",
      "0.025409702\n",
      "48 / 100\n",
      "0.02532146\n",
      "49 / 100\n",
      "0.025408486\n",
      "50 / 100\n",
      "0.02541937\n",
      "51 / 100\n",
      "0.025366955\n",
      "52 / 100\n",
      "0.025260586\n",
      "53 / 100\n",
      "0.0250029\n",
      "54 / 100\n",
      "0.025086233\n",
      "55 / 100\n",
      "0.025144657\n",
      "56 / 100\n",
      "0.025056817\n",
      "57 / 100\n",
      "0.025116654\n",
      "58 / 100\n",
      "0.025088107\n",
      "59 / 100\n",
      "0.024985503\n",
      "60 / 100\n",
      "0.02497721\n",
      "61 / 100\n",
      "0.025091114\n",
      "62 / 100\n",
      "0.024969216\n",
      "63 / 100\n",
      "0.025057053\n",
      "64 / 100\n",
      "0.025022885\n",
      "65 / 100\n",
      "0.02492249\n",
      "66 / 100\n",
      "0.024867062\n",
      "67 / 100\n",
      "0.024811853\n",
      "68 / 100\n",
      "0.025094613\n",
      "69 / 100\n",
      "0.025054125\n",
      "70 / 100\n",
      "0.02487094\n",
      "71 / 100\n",
      "0.0248896\n",
      "72 / 100\n",
      "0.02490798\n",
      "73 / 100\n",
      "0.024951564\n",
      "74 / 100\n",
      "0.024935784\n",
      "75 / 100\n",
      "0.024910219\n",
      "76 / 100\n",
      "0.02482513\n",
      "77 / 100\n",
      "0.024969058\n",
      "78 / 100\n",
      "0.025081191\n",
      "79 / 100\n",
      "0.025104532\n",
      "80 / 100\n",
      "0.024901202\n",
      "81 / 100\n",
      "0.024971602\n",
      "82 / 100\n",
      "0.024813\n",
      "83 / 100\n",
      "0.02487771\n",
      "84 / 100\n",
      "0.025038935\n",
      "85 / 100\n",
      "0.02492732\n",
      "86 / 100\n",
      "0.02476237\n",
      "87 / 100\n",
      "0.024793852\n",
      "88 / 100\n",
      "0.024761703\n",
      "89 / 100\n",
      "0.02462287\n",
      "90 / 100\n",
      "0.024718216\n",
      "91 / 100\n",
      "0.024797903\n",
      "92 / 100\n",
      "0.024552746\n",
      "93 / 100\n",
      "0.024674116\n",
      "94 / 100\n",
      "0.024662435\n",
      "95 / 100\n",
      "0.024683403\n",
      "96 / 100\n",
      "0.024568439\n",
      "97 / 100\n",
      "0.024604626\n",
      "98 / 100\n",
      "0.024698272\n",
      "99 / 100\n",
      "0.024429258\n",
      "100 / 100\n",
      "0.024549957\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(item_infomation_matrix) #random index of train data\n",
    "\n",
    "item_infomation_matrix_noise = add_noise(item_infomation_matrix , 0.3)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    print(\"%d / %d\"%(epoch+1 , epochs))\n",
    "    \n",
    "    for i in range(0 , item_infomation_matrix.shape[0] , batch_size):\n",
    "        X_train_batch = item_infomation_matrix_noise[i:i+batch_size]\n",
    "        y_train_batch = item_infomation_matrix[i:i+batch_size]\n",
    "        \n",
    "        _ , my_loss = sess.run([optimizer, loss] , feed_dict={X_0 :X_train_batch , X_c : y_train_batch , drop_ratio : 0.1})\n",
    "    print(my_loss)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
